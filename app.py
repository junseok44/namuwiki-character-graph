"""ë“±ì¥ì¸ë¬¼ ê´€ê³„ ê·¸ë˜í”„ ì›¹ ì„œë²„"""
import os
import sys
import json
from flask import Flask, request, jsonify, render_template
from flask_cors import CORS

# í˜„ì¬ í”„ë¡œì íŠ¸ í´ë”ì˜ ë°ì´í„° ê²½ë¡œ
BASE_DIR = os.path.dirname(__file__)
DEFAULT_DATA_PATH = os.path.join(BASE_DIR, 'data')
DATASET_PATH = os.environ.get('DATA_DIR', DEFAULT_DATA_PATH)

# ì¸ë±ìŠ¤ íŒŒì¼ ê²½ë¡œë„ DATASET_PATHë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì„¤ì •
INDEX_CACHE_FILE = os.path.join(DATASET_PATH, 'title_index_cache.pkl')

# í˜„ì¬ í”„ë¡œì íŠ¸ì˜ modules ì‚¬ìš©
from modules.namuwiki_dataset import (
    load_namuwiki_dataset,
    get_data_from_dataset,
    build_title_index,
)
from modules.document_search import (
    find_document_by_exact_title_indexed,
    find_document_by_keyword_included,
    find_most_similar_document,
)
from modules.image_extractor import extract_all_image_urls
from modules.character_extractor import extract_character_names_with_ai
from modules.graph_generator import extract_character_relationships_with_ai
from modules.ai_service import reset_ai_request_stats
from modules.namuwiki_web import fetch_namuwiki_page

app = Flask(__name__)
CORS(app)

# ì „ì—­ ë³€ìˆ˜: ì„œë²„ ì‹œì‘ ì‹œ ë¡œë“œëœ ë°ì´í„°ì…‹ê³¼ ì¸ë±ìŠ¤
dataset = None
data = None
title_to_indices = None
title_list = None


def load_dataset_and_index():
    """ì„œë²„ ì‹œì‘ ì‹œ ë°ì´í„°ì…‹ê³¼ ì¸ë±ìŠ¤ë¥¼ ë©”ëª¨ë¦¬ì— ë¡œë“œ"""
    global dataset, data, title_to_indices, title_list
    
    print(f"ë°ì´í„°ì…‹ ê²½ë¡œ: {DATASET_PATH}")
    print(f"ì¸ë±ìŠ¤ ìºì‹œ íŒŒì¼: {INDEX_CACHE_FILE}")
    
    # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜í•˜ì—¬ ì „ë‹¬
    dataset_abs_path = os.path.abspath(DATASET_PATH)
    dataset = load_namuwiki_dataset(dataset_abs_path)
    data = get_data_from_dataset(dataset)
    print(f"ì´ ë¬¸ì„œ ìˆ˜: {len(data)}")
    
    print("ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
    index_cache_abs_path = os.path.abspath(INDEX_CACHE_FILE)
    title_to_indices, title_list = build_title_index(
        data, cache_file=index_cache_abs_path, force_rebuild=False
    )
    print("ë°ì´í„°ì…‹ ë° ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ!")

try:
    load_dataset_and_index()
except Exception as e:
    # ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ ì‹œ ì„œë²„ë¥¼ ì¢…ë£Œí•˜ê±°ë‚˜ ê²½ê³ ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
    print(f"ğŸš¨ğŸš¨ ì¹˜ëª…ì ì¸ ì—ëŸ¬: ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨! {e}")
    # í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œëŠ” sys.exit(1)ë¡œ ì„œë²„ ë¶€íŒ…ì„ ë§‰ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    # sys.exit(1)

@app.route('/')
def index():
    """ë©”ì¸ í˜ì´ì§€"""
    return render_template('index.html')


@app.route('/api/extract-characters', methods=['POST'])
def extract_characters():
    """ì‘í’ˆëª…ì„ ë°›ì•„ì„œ ì¸ë¬¼ëª… ì¶”ì¶œ"""
    try:
        req_data = request.get_json()
        keyword = req_data.get('keyword')
        
        if not keyword:
            return jsonify({'error': 'keywordê°€ í•„ìš”í•©ë‹ˆë‹¤.'}), 400
        
        print(f"\n[ì¸ë¬¼ ì¶”ì¶œ] í‚¤ì›Œë“œ: {keyword}")
        
        # AI ìš”ì²­ í†µê³„ ì´ˆê¸°í™”
        reset_ai_request_stats()
        
        # 1. ë‘ ê°œì˜ ë…ë¦½ì ì¸ í”„ë¡œì„¸ìŠ¤ë¡œ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ ì°¾ê¸°
        
        # í”„ë¡œì„¸ìŠ¤ 1: keywordì™€ ê°€ì¥ ìœ ì‚¬í•œ ë©”ì¸ ë¬¸ì„œ ì°¾ê¸°
        main_doc_idx, main_doc, matched_main_title, main_similarity = find_most_similar_document(
            title_list, title_to_indices, data, keyword, suffix=None, verbose=False
        )
        
        if main_doc_idx is None:
            return jsonify({'error': f"'{keyword}' ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}), 404
        
        # í”„ë¡œì„¸ìŠ¤ 2: keywordì™€ ê°€ì¥ ìœ ì‚¬í•œ ë“±ì¥ì¸ë¬¼ ë¬¸ì„œ ì°¾ê¸°
        char_doc_idx, char_doc, matched_char_title, char_similarity = find_most_similar_document(
            title_list, title_to_indices, data, keyword, suffix="/ë“±ì¥ì¸ë¬¼", verbose=False
        )
        
        if char_doc_idx is None:
            char_doc = {'title': '', 'text': ''}
            matched_char_title = None
            char_similarity = 0.0
        
        # 2. AIì—ê²Œ ë‘ ë¬¸ì„œ ë‚´ìš© ë³´ë‚´ì„œ ì¸ë¬¼ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ (ìµœëŒ€ 20ëª…)
        main_doc_text = main_doc.get('text', '')
        char_list_doc_text = char_doc.get('text', '')
        
        character_names = extract_character_names_with_ai(
            keyword, main_doc_text, char_list_doc_text, max_characters=20
        )
        
        if not character_names:
            return jsonify({'error': 'ì¶”ì¶œëœ ì¸ë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.'}), 404
        
        print(f"âœ… ì¶”ì¶œëœ ì¸ë¬¼ ({len(character_names)}ëª…): {character_names}")
        
        return jsonify({
            'success': True,
            'characters': character_names,
            'main_document': {
                'title': main_doc.get('title', ''),
                'index': main_doc_idx
            },
            'character_list_document': {
                'title': char_doc.get('title', ''),
                'index': char_doc_idx if char_doc_idx else None
            }
        })
        
    except Exception as e:
        print(f"ì—ëŸ¬ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500


@app.route('/api/search-document', methods=['POST'])
def search_document():
    """
    keywordë¡œ ê°€ì¥ ìœ ì‚¬í•œ ë‚˜ë¬´ìœ„í‚¤ ë¬¸ì„œë¥¼ ì°¾ì•„
    ì œëª©ê³¼ ë‚´ìš© ì¼ë¶€ë¥¼ ë°˜í™˜í•˜ëŠ” í…ŒìŠ¤íŠ¸ìš© API
    """
    try:
        req_data = request.get_json()
        keyword = req_data.get('keyword') if req_data else None
        suffix = req_data.get('suffix') if req_data else None
        max_preview_chars = req_data.get('max_preview_chars', 500) if req_data else 500

        if not keyword:
            return jsonify({'error': 'keywordê°€ í•„ìš”í•©ë‹ˆë‹¤.'}), 400

        print(f"\n[ë¬¸ì„œ ê²€ìƒ‰] í‚¤ì›Œë“œ: {keyword}, suffix: {suffix}")

        # ìœ ì‚¬ë„ ê¸°ë°˜ ë¬¸ì„œ ê²€ìƒ‰
        best_idx, best_doc, matched_title, similarity = find_most_similar_document(
            title_list, title_to_indices, data, keyword, suffix=suffix, verbose=False
        )

        if best_idx is None or not best_doc:
            return jsonify({'error': f"'{keyword}' ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}), 404

        full_text = best_doc.get('text', '') or ''
        preview = full_text[:max_preview_chars]

        return jsonify({
            'success': True,
            'keyword': keyword,
            'suffix': suffix,
            'index': best_idx,
            'original_title': best_doc.get('title', ''),
            'matched_title': matched_title,
            'similarity': similarity,
            'preview': preview,
            'full_text_length': len(full_text),
        })

    except Exception as e:
        print(f"ì—ëŸ¬ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500


@app.route('/api/crawl-documents', methods=['POST'])
def crawl_documents():
    """ì¸ë¬¼ëª… ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ì„œ ë‚˜ë¬´ìœ„í‚¤ì—ì„œ ë¬¸ì„œ í¬ë¡¤ë§"""
    try:
        req_data = request.get_json()
        character_names = req_data.get('character_names', [])
        
        if not character_names:
            return jsonify({'error': 'character_namesê°€ í•„ìš”í•©ë‹ˆë‹¤.'}), 400
        
        # ìµœëŒ€ 20ëª…ìœ¼ë¡œ ì œí•œ
        max_characters = 20
        if len(character_names) > max_characters:
            character_names = character_names[:max_characters]
            print(f"âš ï¸  ì¸ë¬¼ ìˆ˜ê°€ {max_characters}ëª…ì„ ì´ˆê³¼í•˜ì—¬ {max_characters}ëª…ìœ¼ë¡œ ì œí•œí–ˆìŠµë‹ˆë‹¤.")
        
        print(f"\n[ë¬¸ì„œ í¬ë¡¤ë§] ì¸ë¬¼ ìˆ˜: {len(character_names)}")
        
        documents = []
        for i, char_name in enumerate(character_names, 1):
            print(f"  [{i}/{len(character_names)}] '{char_name}' í¬ë¡¤ë§ ì¤‘...")
            doc = fetch_namuwiki_page(char_name)
            if doc:
                doc['type'] = 'character'
                doc['source'] = 'web'
                documents.append(doc)
                print(f"    âœ… í¬ë¡¤ë§ ì„±ê³µ: '{doc['title']}'")
            else:
                print(f"    âš ï¸  í¬ë¡¤ë§ ì‹¤íŒ¨: '{char_name}'")
        
        return jsonify({
            'success': True,
            'documents': documents,
            'crawled_count': len(documents),
            'failed_count': len(character_names) - len(documents)
        })
        
    except Exception as e:
        print(f"ì—ëŸ¬ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500


@app.route('/api/generate-graph', methods=['POST'])
def generate_graph():
    """ë¬¸ì„œë“¤ì„ ë°›ì•„ì„œ ê´€ê³„ë„ ìƒì„±"""
    try:
        req_data = request.get_json()
        keyword = req_data.get('keyword')
        character_documents = req_data.get('character_documents', [])  # í´ë¼ì´ì–¸íŠ¸ì—ì„œ í¬ë¡¤ë§í•œ ë¬¸ì„œë“¤
        character_names = req_data.get('character_names', [])
        model = req_data.get('model', 'gpt-4o-mini')  # ê¸°ë³¸ê°’: gpt-4o-mini
        
        # ëª¨ë¸ ê²€ì¦
        if model not in ['gpt-4o-mini', 'gpt-5']:
            return jsonify({'error': 'ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì…ë‹ˆë‹¤. gpt-4o-mini ë˜ëŠ” gpt-5ë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.'}), 400
        
        if not keyword:
            return jsonify({'error': 'keywordê°€ í•„ìš”í•©ë‹ˆë‹¤.'}), 400
        
        print(f"\n[ê´€ê³„ë„ ìƒì„±] í‚¤ì›Œë“œ: {keyword}, ë¬¸ì„œ ìˆ˜: {len(character_documents)}")
        
        # AI ìš”ì²­ í†µê³„ ì´ˆê¸°í™”
        reset_ai_request_stats()
        
        all_documents = []
        
        # ë©”ì¸ ë¬¸ì„œì™€ ë“±ì¥ì¸ë¬¼ ëª©ë¡ ë¬¸ì„œ ì°¾ê¸° (ìœ ì‚¬ë„ ê¸°ë°˜)
        # í”„ë¡œì„¸ìŠ¤ 1: ë©”ì¸ ë¬¸ì„œ ì°¾ê¸°
        main_doc_idx, main_doc, matched_main_title, main_similarity = find_most_similar_document(
            title_list, title_to_indices, data, keyword, suffix=None, verbose=False
        )
        
        if main_doc:
            main_doc_text = main_doc.get('text', '')
            main_image_urls = extract_all_image_urls(main_doc_text)
            all_documents.append({
                'title': main_doc.get('title', ''),
                'text': main_doc_text,
                'image_urls': main_image_urls,
                'type': 'main'
            })
        else:
            print(f"âš ï¸  ë©”ì¸ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # í”„ë¡œì„¸ìŠ¤ 2: ë“±ì¥ì¸ë¬¼ ëª©ë¡ ë¬¸ì„œ ì°¾ê¸°
        char_doc_idx, char_list_doc, matched_char_title, char_similarity = find_most_similar_document(
            title_list, title_to_indices, data, keyword, suffix="/ë“±ì¥ì¸ë¬¼", verbose=False
        )
        
        if char_list_doc and char_list_doc.get('text'):
            char_list_doc_text = char_list_doc.get('text', '')
            char_list_image_urls = extract_all_image_urls(char_list_doc_text)
            all_documents.append({
                'title': char_list_doc.get('title', ''),
                'text': char_list_doc_text,
                'image_urls': char_list_image_urls,
                'type': 'character_list'
            })
        
        # í´ë¼ì´ì–¸íŠ¸ì—ì„œ í¬ë¡¤ë§í•œ ë¬¸ì„œë“¤ ì¶”ê°€
        found_characters = []
        for doc in character_documents:
            all_documents.append(doc)
            found_characters.append(doc.get('title', ''))
        
        # ì›¹ í¬ë¡¤ë§ ì‹¤íŒ¨í•œ ì¸ë¬¼ ë¬¸ì„œë¥¼ ë°ì´í„°ì…‹ì—ì„œ ì°¾ê¸°
        crawled_titles = {doc.get('title', '') for doc in character_documents}
        missing_characters = [name for name in character_names if name not in crawled_titles]
        
        if missing_characters:
            print(f"âš ï¸  ì›¹ í¬ë¡¤ë§ ì‹¤íŒ¨í•œ ì¸ë¬¼ ({len(missing_characters)}ëª…): {missing_characters}")
            print("ë°ì´í„°ì…‹ì—ì„œ ì°¾ëŠ” ì¤‘...")
            
            for char_name in missing_characters:
                char_doc_idx, char_doc = find_document_by_exact_title_indexed(
                    title_to_indices, data, char_name
                )
                
                if char_doc_idx is not None and char_doc:
                    char_doc_text = char_doc.get('text', '')
                    char_image_urls = extract_all_image_urls(char_doc_text)
                    all_documents.append({
                        'title': char_doc.get('title', ''),
                        'text': char_doc_text,
                        'image_urls': char_image_urls,
                        'type': 'character',
                        'source': 'dataset'
                    })
                    found_characters.append(char_doc.get('title', ''))
                    print(f"âœ… ë°ì´í„°ì…‹ì—ì„œ ì°¾ìŒ: {char_doc.get('title', '')}")
        
        print(f"\nâœ… ì´ {len(all_documents)}ê°œì˜ ë¬¸ì„œë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
        
        # 4. ëª¨ë“  ë¬¸ì„œ í•©ì³ì„œ AIì—ê²Œ ê´€ê³„ ê·¸ë˜í”„ ìš”ì²­
        print(f"AIë¥¼ ì‚¬ìš©í•œ ê´€ê³„ ê·¸ë˜í”„ ìƒì„± ì¤‘... (ëª¨ë¸: {model})")
        graph_data = extract_character_relationships_with_ai(keyword, all_documents, model=model)
        
        return jsonify({
            'success': True,
            'graph': graph_data,
            'found_characters': found_characters,
            'total_documents': len(all_documents)
        })
        
    except Exception as e:
        print(f"ì—ëŸ¬ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500


if __name__ == '__main__':
    # ì„œë²„ ì‹œì‘ ì‹œ ë°ì´í„°ì…‹ê³¼ ì¸ë±ìŠ¤ ë¡œë“œ
    load_dataset_and_index()
    
    # Flask ì„œë²„ ì‹¤í–‰
    app.run(debug=True, host='0.0.0.0', port=5000)


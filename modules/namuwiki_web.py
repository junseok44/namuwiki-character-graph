"""ë‚˜ë¬´ìœ„í‚¤ ì›¹ í¬ë¡¤ë§ ëª¨ë“ˆ"""
import urllib.parse
import requests
from bs4 import BeautifulSoup
from typing import Optional, Dict, Any
import time


def build_namuwiki_url(title: str) -> str:
    """
    ë‚˜ë¬´ìœ„í‚¤ ë¬¸ì„œ URL ìƒì„±
    
    Args:
        title: ë¬¸ì„œ ì œëª© (ì˜ˆ: "ì‚°(ëª¨ë…¸ë…¸ì¼€ íˆë©”)")
    
    Returns:
        ë‚˜ë¬´ìœ„í‚¤ URL (ì˜ˆ: "https://namu.wiki/w/%EC%82%B0(%EB%AA%A8%EB%85%B8%EB%85%B8%EC%BC%80%20%ED%9E%88%EB%A9%94)")
    """
    # URL ì¸ì½”ë”©
    encoded_title = urllib.parse.quote(title, safe='')
    return f"https://namu.wiki/w/{encoded_title}"


def fetch_namuwiki_page(title: str, timeout: int = 10) -> Optional[Dict[str, Any]]:
    """
    ë‚˜ë¬´ìœ„í‚¤ í˜ì´ì§€ë¥¼ ê°€ì ¸ì™€ì„œ íŒŒì‹±
    
    Args:
        title: ë¬¸ì„œ ì œëª©
        timeout: ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
    
    Returns:
        {'title': ì œëª©, 'text': í…ìŠ¤íŠ¸ ë‚´ìš©, 'image_src': ì´ë¯¸ì§€ URL} ë˜ëŠ” None
    """
    url = build_namuwiki_url(title)
    
    try:
        # User-Agent ì„¤ì • (ë´‡ ì°¨ë‹¨ ë°©ì§€)
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        print(f"    ğŸŒ ì›¹ì—ì„œ ê°€ì ¸ì˜¤ëŠ” ì¤‘: {url}")
        response = requests.get(url, headers=headers, timeout=timeout)
        response.raise_for_status()
        
        # HTML íŒŒì‹±
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ì œëª© ì¶”ì¶œ (h1 íƒœê·¸ì—ì„œ ì°¾ê¸°, í´ë˜ìŠ¤ëª…ì´ í•´ì‹œë˜ì–´ ìˆìœ¼ë¯€ë¡œ íƒœê·¸ë§Œìœ¼ë¡œ ì°¾ê¸°)
        title_elem = soup.find('h1')
        if title_elem:
            page_title = title_elem.get_text(strip=True)
        else:
            page_title = title
        
        # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (id="app" divì—ì„œ ì°¾ê¸°)
        content_elem = soup.find(id='app')
        if not content_elem:
            print(f"    âš ï¸  ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # ìŠ¤í¬ë¦½íŠ¸ì™€ ìŠ¤íƒ€ì¼ ì œê±°
        for script in content_elem(["script", "style"]):
            script.decompose()
        
        # í…ìŠ¤íŠ¸ ë‚´ìš© ì¶”ì¶œ
        text_content = content_elem.get_text(separator='\n', strip=True)
        
        # ì´ë¯¸ì§€ URL ì¶”ì¶œ (ë³„ë„ë¡œ ì €ì¥)
        image_urls = []
        img_tags = content_elem.find_all('img')
        for img in img_tags:
            src = img.get('src') or img.get('data-src') or img.get('data-original')
            if src:
                # ë‚˜ë¬´ìœ„í‚¤ ì´ë¯¸ì§€ ì„œë²„ URLì¸ì§€ í™•ì¸
                if 'namu.wiki' in src or 'namu.la' in src or 'i.namu.wiki' in src:
                    # ìƒëŒ€ ê²½ë¡œì¸ ê²½ìš° ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                    if src.startswith('//'):
                        full_url = 'https:' + src
                    elif src.startswith('/'):
                        full_url = 'https://namu.wiki' + src
                    elif src.startswith('http://') or src.startswith('https://'):
                        full_url = src
                    else:
                        full_url = 'https://namu.wiki' + src
                    
                    # ë¡œê³ ë‚˜ ì•„ì´ì½˜ ì œì™¸
                    if not any(exclude in full_url.lower() for exclude in ['logo', 'icon', 'button', 'spacer']):
                        alt_text = img.get('alt', '')
                        # img íƒœê·¸ ì£¼ë³€ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ìœ„ì¹˜ ì •ë³´)
                        parent = img.find_parent()
                        context_text = ""
                        if parent:
                            # ë¶€ëª¨ ìš”ì†Œì˜ í…ìŠ¤íŠ¸ ì¼ë¶€ ì¶”ì¶œ
                            parent_text = parent.get_text(separator=' ', strip=True)
                            context_text = parent_text[:200]  # ìµœëŒ€ 200ì
                        
                        image_urls.append({
                            'url': full_url,
                            'alt': alt_text,
                            'context': context_text
                        })
        
        return {
            'title': page_title,
            'text': text_content,  # í…ìŠ¤íŠ¸ë§Œ
            'image_urls': image_urls,  # ì´ë¯¸ì§€ URL ë¦¬ìŠ¤íŠ¸ ë³„ë„ ì €ì¥
            'image_src': image_urls[0]['url'] if image_urls else None,  # í•˜ìœ„ í˜¸í™˜ì„±
            'url': url
        }
        
    except requests.exceptions.RequestException as e:
        print(f"    âŒ ì›¹ ìš”ì²­ ì‹¤íŒ¨: {e}")
        return None
    except Exception as e:
        print(f"    âŒ íŒŒì‹± ì‹¤íŒ¨: {e}")
        return None


def fetch_character_documents(character_names: list, delay: float = 0.5) -> list:
    """
    ì—¬ëŸ¬ ì¸ë¬¼ì˜ ë‚˜ë¬´ìœ„í‚¤ ë¬¸ì„œë¥¼ ì›¹ì—ì„œ ê°€ì ¸ì˜¤ê¸°
    
    Args:
        character_names: ì¸ë¬¼ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        delay: ìš”ì²­ ê°„ ì§€ì—° ì‹œê°„ (ì´ˆ, ì„œë²„ ë¶€í•˜ ë°©ì§€)
    
    Returns:
        ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (ê°ê° title, text, image_src í¬í•¨)
    """
    documents = []
    
    for i, character_name in enumerate(character_names, 1):
        print(f"  [{i}/{len(character_names)}] '{character_name}' ì›¹ì—ì„œ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
        
        doc = fetch_namuwiki_page(character_name)
        if doc:
            doc['type'] = 'character'
            documents.append(doc)
            print(f"    âœ… ë¬¸ì„œ ê°€ì ¸ì˜´: '{doc['title']}'")
            if doc.get('image_src'):
                print(f"    ğŸ“· ì´ë¯¸ì§€: {doc['image_src'][:80]}...")
        else:
            print(f"    âš ï¸  ë¬¸ì„œë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ì§€ì—°
        if i < len(character_names):
            time.sleep(delay)
    
    return documents


def fetch_and_merge_character_documents(
    character_names: list,
    title_to_indices: dict,
    data,
    delay: float = 0.5
) -> list:
    """
    ì—¬ëŸ¬ ì¸ë¬¼ì˜ ë‚˜ë¬´ìœ„í‚¤ ë¬¸ì„œë¥¼ ì›¹ì—ì„œ ìš°ì„  ê°€ì ¸ì˜¤ê³ , ì‹¤íŒ¨í•˜ë©´ ë°ì´í„°ì…‹ì—ì„œ ê°€ì ¸ì˜¤ê¸°
    
    Args:
        character_names: ì¸ë¬¼ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        title_to_indices: ì œëª© ì¸ë±ìŠ¤ ë”•ì…”ë„ˆë¦¬
        data: ë°ì´í„°ì…‹ ë°ì´í„°
        delay: ì›¹ ìš”ì²­ ê°„ ì§€ì—° ì‹œê°„ (ì´ˆ, ì„œë²„ ë¶€í•˜ ë°©ì§€)
    
    Returns:
        ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (ê°ê° title, text, image_urls í¬í•¨)
        - ì›¹ì—ì„œ ì„±ê³µí•˜ë©´ ì›¹ ë¬¸ì„œë§Œ ì‚¬ìš©
        - ì›¹ì—ì„œ ì‹¤íŒ¨í•˜ë©´ ë°ì´í„°ì…‹ ë¬¸ì„œ ì‚¬ìš©
        - ë‘˜ ë‹¤ ì‹¤íŒ¨í•˜ë©´ í•´ë‹¹ ì¸ë¬¼ì€ ì œì™¸
    """
    from .document_search import search_document_by_title_indexed
    from .image_extractor import extract_all_image_urls
    
    documents = []
    
    for i, character_name in enumerate(character_names, 1):
        print(f"  [{i}/{len(character_names)}] '{character_name}' ë¬¸ì„œ ìˆ˜ì§‘ ì¤‘...")
        
        # 1. ë°ì´í„°ì…‹ì—ì„œ ê²€ìƒ‰
        dataset_doc_idx, dataset_doc = search_document_by_title_indexed(
            title_to_indices, data, character_name
        )
        
        dataset_text = ""
        dataset_image_urls = []
        if dataset_doc:
            dataset_text = dataset_doc.get('text', '')
            dataset_image_urls = extract_all_image_urls(dataset_text)
            print(f"    ğŸ“š ë°ì´í„°ì…‹ì—ì„œ ì°¾ìŒ: '{dataset_doc.get('title', '')}' ({len(dataset_text)}ì, ì´ë¯¸ì§€ {len(dataset_image_urls)}ê°œ)")
        else:
            print(f"    âš ï¸  ë°ì´í„°ì…‹ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        
        # 2. ì›¹ì—ì„œ ê°€ì ¸ì˜¤ê¸° (ìš°ì„  ì‹œë„)
        web_doc = fetch_namuwiki_page(character_name)
        
        # 3. ì›¹ì—ì„œ ì„±ê³µí•˜ë©´ ì›¹ ë¬¸ì„œ ì‚¬ìš©, ì‹¤íŒ¨í•˜ë©´ ë°ì´í„°ì…‹ ì‚¬ìš©
        final_doc = None
        if web_doc:
            # ì›¹ì—ì„œ ì„±ê³µí•œ ê²½ìš°
            web_text = web_doc.get('text', '')
            web_image_urls = web_doc.get('image_urls', [])
            web_title = web_doc.get('title', character_name)
            
            final_doc = {
                'title': web_title,
                'text': web_text,
                'image_urls': web_image_urls,
                'type': 'character',
                'source': 'web'
            }
            print(f"    âœ… ì›¹ ë¬¸ì„œ ì‚¬ìš©: '{web_title}' ({len(web_text)}ì, ì´ë¯¸ì§€ {len(web_image_urls)}ê°œ)")
        elif dataset_doc:
            # ì›¹ì—ì„œ ì‹¤íŒ¨í•˜ê³  ë°ì´í„°ì…‹ì— ìˆëŠ” ê²½ìš°
            dataset_title = dataset_doc.get('title', character_name)
            final_doc = {
                'title': dataset_title,
                'text': dataset_text,
                'image_urls': dataset_image_urls,
                'type': 'character',
                'source': 'dataset'
            }
            print(f"    âœ… ë°ì´í„°ì…‹ ë¬¸ì„œ ì‚¬ìš©: '{dataset_title}' ({len(dataset_text)}ì, ì´ë¯¸ì§€ {len(dataset_image_urls)}ê°œ)")
        
        if final_doc:
            documents.append(final_doc)
        else:
            print(f"    âŒ ë¬¸ì„œë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ì§€ì—°
        if i < len(character_names):
            time.sleep(delay)
    
    return documents

